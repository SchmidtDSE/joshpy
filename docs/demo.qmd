---
title: "joshpy Parameter Sweep Demo"
format: html
execute:
  warning: false
---

```{r}
#| label: setup
#| include: false
library(reticulate)
library(ggplot2)
library(dplyr)
```

## Introduction

[Josh](https://joshsim.org) is an ecological simulation runtime for agent-based
modeling developed by the [Eric and Wendy Schmidt Center for Data Science and Environment](https://github.com/SchmidtDSE/josh).
This demo assumes familiarity with Josh's simulation language and runtime.

**joshpy** is a Python client that enables:

- **Orchestration**: Define parameter sweeps, expand job configurations, and
  execute simulations programmatically
- **Tracking**: Register runs in a DuckDB-backed registry with session and
  config tracking
- **Data Loading**: Import cell-level CSV exports into queryable tables
- **Analysis**: Query results across parameter values and replicates
- **Diagnostics**: Quick matplotlib visualizations for simulation sanity checks
- **Visualization**: Create publication-quality plots with R/ggplot2 integration

This demo walks through a complete parameter sweep workflow. We vary the
`maxGrowth` parameter from 10 to 100 meters/step across 10 experiments, each
with 3 replicates, then load, query, and visualize the results.

## Prerequisites

Ensure the Josh JAR is available at `jar/joshsim-fat.jar` and joshpy is installed:

```bash
pip install -e '.[all]'
```

For visualization, ensure R is installed with the following packages:

```r
install.packages(c("reticulate", "ggplot2", "dplyr"))
```

## Step 1: Setup - Define Parameter Sweep

The first step is to define our experiment configuration. joshpy uses three key
abstractions:

- **`JobConfig`**: The top-level configuration specifying source files, templates,
  and sweep parameters
- **`SweepConfig`**: Defines which parameters to sweep and their values
- **`SweepParameter`**: A single parameter with a name and list of values

The `JobExpander` will later compute the cartesian product of all parameters,
generating one job per combination.

```{python}
from pathlib import Path

from joshpy.jobs import JobConfig, SweepConfig, SweepParameter

# Paths to source files
SOURCE_PATH = Path("../examples/hello_cli_configurable.josh")
TEMPLATE_PATH = Path("../examples/templates/sweep_config.jshc.j2")

# Parameter sweep: maxGrowth from 10 to 100 in steps of 10
MAX_GROWTH_VALUES = list(range(10, 101, 10))

config = JobConfig(
    template_path=TEMPLATE_PATH,
    source_path=SOURCE_PATH,
    simulation="Main",
    replicates=3,
    sweep=SweepConfig(
        parameters=[SweepParameter(name="maxGrowth", values=MAX_GROWTH_VALUES)]
    ),
)

print(f"Parameter values: {MAX_GROWTH_VALUES}")
print(f"Replicates per job: {config.replicates}")
print(f"Total runs: {len(MAX_GROWTH_VALUES)} x {config.replicates} = {len(MAX_GROWTH_VALUES) * config.replicates}")
```

Let's examine the source files. The `.josh` file defines the simulation, and
the `.jshc.j2` template provides parameterized configuration:

#### Josh Source

```{python}
print(SOURCE_PATH.read_text())
```

#### Template Configuration

```{python}
print(TEMPLATE_PATH.read_text())
```

Notice that the `.josh` file references `config sweep_config.maxGrowth` - this
pulls the value from our generated config file at runtime.

## Step 2: Initialize Registry and Expand Jobs

The `RunRegistry` provides experiment tracking backed by DuckDB. It stores:

- **Sessions**: High-level experiment metadata
- **Configs**: Rendered configuration files with parameter values
- **Runs**: Individual execution records with timing and exit codes

The `JobExpander` takes our `JobConfig` and generates concrete jobs - one per
parameter combination, each with a unique config hash for tracking.

```{python}
from joshpy.jobs import JobExpander
from joshpy.registry import RunRegistry

# Create in-memory registry for this demo
registry = RunRegistry(":memory:")

# Create a session to track this experiment
session_id = registry.create_session(
    experiment_name="growth_rate_sweep",
    simulation="Main",
    total_jobs=len(MAX_GROWTH_VALUES),
    total_replicates=len(MAX_GROWTH_VALUES) * config.replicates,
    template_path=str(TEMPLATE_PATH),
)
print(f"Session ID: {session_id}")

# Expand config into individual jobs
expander = JobExpander()
job_set = expander.expand(config)
print(f"Expanded to {len(job_set)} jobs")

# Register each job's config in the registry
for job in job_set.jobs:
    registry.register_config(
        session_id=session_id,
        config_hash=job.config_hash,
        config_content=job.config_content,
        parameters=job.parameters,
    )
    print(f"  maxGrowth={job.parameters['maxGrowth']:>3} -> hash={job.config_hash}")
```

## Step 3: Run the Simulations

The `JoshCLI` executes jobs via the Josh command-line interface. The `to_run_config()`
helper converts an `ExpandedJob` to a `RunConfig` dataclass that maps directly to
CLI arguments:

- Source file and simulation name
- Config file path (via `--data` flag)
- Custom tags for template variable resolution (via `--custom-tag`)
- Replicate count

The `RegistryCallback` automatically records run metadata as each job completes.

```{python}
from joshpy.cli import JoshCLI
from joshpy.jobs import to_run_config
from joshpy.registry import RegistryCallback

# Update session status
registry.update_session_status(session_id, "running")

# Create CLI targeting the local fat JAR
cli = JoshCLI(josh_jar=Path("../jar/joshsim-fat.jar"))
callback = RegistryCallback(registry, session_id)

print("Running 10 experiments (3 replicates each)...")
print("=" * 60)

results = []
for job in job_set.jobs:
    run_config = to_run_config(job)
    result = cli.run(run_config)
    callback.record(job, result)
    results.append((job, result))
    status = "OK" if result.success else "FAIL"
    print(f"[{status}] maxGrowth={job.parameters['maxGrowth']}")
    if not result.success:
        print(f"       Error: {result.stderr[:200]}...")

print("=" * 60)
succeeded = sum(1 for _, r in results if r.success)
print(f"Completed: {succeeded}/{len(results)} succeeded")

# Update final session status
final_status = "completed" if succeeded == len(results) else "failed"
registry.update_session_status(session_id, final_status)
```

## Step 4: Load Cell Data from CSVs

Josh exports simulation data to CSV files. The `CellDataLoader` imports these
into a DuckDB `cell_data` table, linking each row to its originating run via
`run_id` and `config_hash`. This enables queries that join parameters with
results.

```{python}
from joshpy.cell_data import CellDataLoader

loader = CellDataLoader(registry)

print("Loading CSV exports into registry...")
total_loaded = 0
files_found = 0

for job, result in results:
    if not result.success:
        continue

    mg = job.parameters["maxGrowth"]

    # Get run_id from registry (created by RegistryCallback)
    runs = registry.get_runs_for_config(job.config_hash)
    if not runs:
        print(f"  Skipping maxGrowth={mg} (no run_id)")
        continue

    run_id = runs[0].run_id

    # Load each replicate's CSV
    for rep in range(job.replicates):
        csv_path = Path(f"/tmp/hello_josh_{mg}_{rep}.csv")
        if csv_path.exists():
            rows = loader.load_csv(csv_path, run_id=run_id, config_hash=job.config_hash)
            total_loaded += rows
            files_found += 1

print(f"\nLoaded {total_loaded:,} rows from {files_found} files")
```

## Step 5: Diagnostic Plotting (Python)

The `SimulationDiagnostics` class provides quick matplotlib-based visualizations
for simulation sanity checks - useful for verifying that simulations behave as
expected before deeper analysis.

### Discover Available Data

First, let's see what data is available in the registry:

```{python}
# Get summary of loaded data
summary = registry.get_data_summary()
print(summary)
```

We can also query individual lists:

```{python}
print(f"Variables: {registry.list_variables()}")
print(f"Parameters: {registry.list_parameters()}")
print(f"Entity types: {registry.list_entity_types()}")
```

### Time Series Visualization

The `plot_timeseries()` method shows how a variable evolves over simulation steps.
By default, it spatially aggregates across patches and shows uncertainty bands
across replicates.

```{python}
#| label: fig-diag-timeseries
#| fig-cap: "Tree height over time for maxGrowth=50, with uncertainty bands across replicates."

from joshpy.diagnostics import SimulationDiagnostics

diag = SimulationDiagnostics(registry)

# Plot time series for a specific parameter value
# Filter by maxGrowth=50 to see a single experiment
diag.plot_timeseries(
    "averageHeight",
    maxGrowth=50,
    title="Average Tree Height Over Time (maxGrowth=50)",
    show=True,
)
```

### Parameter Comparison

The `plot_comparison()` method compares a variable across different parameter
values - ideal for visualizing parameter sweep results.

```{python}
#| label: fig-diag-comparison
#| fig-cap: "Tree height trajectories across all maxGrowth parameter values."

# Compare averageHeight across all maxGrowth values
diag.plot_comparison(
    "averageHeight",
    group_by="maxGrowth",
    title="Tree Height by Growth Rate Parameter",
    show=True,
)
```

We can also create a bar chart at a specific timestep:

```{python}
#| label: fig-diag-barchart
#| fig-cap: "Final tree height at step 10 for each maxGrowth value."

# Bar chart at final step
diag.plot_comparison(
    "averageHeight",
    group_by="maxGrowth",
    step=10,
    title="Final Tree Height vs Growth Rate",
    show=True,
)
```

### Saving Figures

All plot methods return a matplotlib Figure that can be saved:

```{python}
# Save to file instead of showing inline
fig = diag.plot_comparison(
    "averageHeight",
    group_by="maxGrowth",
    show=False,
)
fig.savefig("/tmp/diagnostic_comparison.png", dpi=150, bbox_inches="tight")
print("Saved to /tmp/diagnostic_comparison.png")
```

## Step 6: Visualize Results (R)

For publication-quality figures or more customized visualizations, we can pass
data to R for visualization using ggplot2. The `DiagnosticQueries` class provides
ready-made queries that return pandas DataFrames, which we can export to CSV for R.

First, let's query the data and export it:

```{python}
from joshpy.cell_data import DiagnosticQueries

queries = DiagnosticQueries(registry)

# Compare averageHeight across all maxGrowth values
df = queries.get_parameter_comparison(
    variable="averageHeight",
    param_name="maxGrowth",
)

print(f"Retrieved {len(df)} rows\n")
if not df.empty:
    print("Sample data (first 15 rows):")
    print(df.head(15).to_string(index=False))
    # Save to CSV for R visualization
    df.to_csv("/tmp/sweep_results.csv", index=False)
    print("\nSaved results to /tmp/sweep_results.csv")
```

Now we can use ggplot2 in R to create publication-quality figures:

1. **Time series**: Tree height over simulation steps with ribbon for uncertainty
2. **Bar chart**: Final height vs growth rate parameter with error bars

```{r}
#| label: fig-timeseries
#| fig-cap: "Tree height trajectories for different maxGrowth parameter values. Higher growth rates produce taller trees, as expected."
#| fig-width: 10
#| fig-height: 6

# Read results from CSV exported by Python
df <- read.csv("/tmp/sweep_results.csv")

if (nrow(df) > 0) {
  p <- ggplot(df, aes(x = step, y = mean_value, color = factor(param_value), fill = factor(param_value))) +
    geom_ribbon(aes(ymin = mean_value - std_value, ymax = mean_value + std_value), alpha = 0.2, color = NA) +
    geom_line(linewidth = 0.8) +
    geom_point(size = 2) +
    labs(
      x = "Simulation Step",
      y = "Average Tree Height (meters)",
      title = "Tree Growth Over Time by maxGrowth Parameter",
      color = "maxGrowth",
      fill = "maxGrowth"
    ) +
    theme_minimal() +
    theme(
      legend.position = "right",
      panel.grid.minor = element_blank()
    )
  print(p)
} else {
  cat("No data to plot.")
}
```

```{r}
#| label: fig-final-height
#| fig-cap: "Final tree height vs growth rate parameter. Error bars show standard deviation across replicates."
#| fig-width: 10
#| fig-height: 6

# Read results from CSV exported by Python
df <- read.csv("/tmp/sweep_results.csv")

if (nrow(df) > 0) {
  final_df <- df |>
    dplyr::filter(step == max(step))

  p <- ggplot(final_df, aes(x = param_value, y = mean_value)) +
    geom_col(fill = "steelblue", alpha = 0.7) +
    geom_errorbar(
      aes(ymin = mean_value - std_value, ymax = mean_value + std_value),
      width = 3,
      linewidth = 0.6
    ) +
    labs(
      x = "maxGrowth (meters/step)",
      y = "Final Average Height (meters)",
      title = "Final Tree Height vs Growth Rate"
    ) +
    theme_minimal() +
    theme(panel.grid.minor = element_blank())
  print(p)

  # Calculate and print correlation
  r <- cor(final_df$param_value, final_df$mean_value)
  cat(sprintf("\nCorrelation between maxGrowth and final height: r = %.4f\n", r))
} else {
  cat("No data to plot.")
}
```

## Summary

This demo illustrated the core joshpy workflow:

1. **Define** a parameter sweep using `JobConfig` and `SweepConfig`
2. **Track** experiments with `RunRegistry` sessions and configs
3. **Execute** simulations via `JobRunner` with automatic tracking
4. **Load** CSV outputs into queryable DuckDB tables
5. **Query** results across parameters using `DiagnosticQueries`
6. **Diagnose** simulation behavior with `SimulationDiagnostics` (Python/matplotlib)
7. **Visualize** publication-quality figures with ggplot2 (R)

The registry-backed approach ensures reproducibility and enables complex
queries joining parameters with results across multiple experiments.

**Diagnostic Plotting**: The `SimulationDiagnostics` class provides quick
matplotlib-based visualizations for sanity checks during development:

- `plot_timeseries()`: Variable evolution over time with uncertainty bands
- `plot_comparison()`: Compare across parameter values (time series or bar chart)
- `plot_spatial()`: Spatial snapshots at specific timesteps

**Discovery Methods**: Before plotting, use `registry.get_data_summary()`,
`registry.list_variables()`, and `registry.list_parameters()` to see what
data is available.

This document also demonstrates Quarto's ability to seamlessly combine R and
Python in the same workflow using `reticulate`. Python handles the simulation
orchestration, data querying, and quick diagnostics, while R handles
publication-quality visualization with ggplot2.

## Cleanup

```{python}
registry.close()
print("Registry closed.")
```
